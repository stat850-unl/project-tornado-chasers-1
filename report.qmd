---
format: html
editor: visual
---

```{r, eval = F}
# This code chunk contains code to install all of the dependencies
# necessary to compile and run your report, using if-statements to
# reduce install time for unnecessary code.
# It should be set to eval = F by default, so you aren't installing
# software on someone's computer without their consent.

# This works for packages that are on CRAN
if (!"dplyr" %in% installed.packages()) {
  install.packages("dplyr")
}
if (!"remotes" %in% installed.packages()) {
  install.packages("remotes")
}

# This is how to do it for a package that's only on github
if (!"emo" %in% installed.packages()) {
  remotes::install_github("hadley/emo")

```

```{r}

```

## Introduction

Introduce your dataset and basic questions for exploration. Explain any unique approaches you will take or any interesting features of your dataset that you have to overcome. Cite the source of your dataset, and briefly discuss how it was collected.

Discuss the variables in the dataset and if there are anomalies in the variables, provide some visual assessment of the anomalies and explain how they may have arisen. You should cover most of the information in your proposal data section, but should primarily use paragraphs and not lists/tables (the exception may be a list of e.g. items which were measured).

A tornado is a rapidly rotating column of air that extends from a thunderstorm to the ground, forming a funnel-shaped cloud, known as a tornado vortex (Tornadoes.(n.d.)). These violent windstorms are characterized by their destructive power, often causing severe damage to structures, vehicles, and natural environments. So this most destructive and fascinating natural phenomena that occur in the United States, with the power to wreak havoc on communities, devastate landscapes, and claim lives. They are often referred to as nature's most violent storms due to their ferocity and unpredictability. In the USA, tornadoes have long been a subject of both fear and awe, prompting researchers, meteorologists, and emergency response teams to seek a deeper understanding of these formidable weather events.

The interest in exploring this extensive tornado data set arises from the profound impact that tornadoes have on American society and the natural environment. Tornadoes are a recurring and unpredictable threat, causing significant economic losses and, more importantly, putting human lives at risk. As a researcher, I am driven by the need to better understand the behavior and patterns of tornadoes to mitigate their destructive potential and improve our ability to predict and respond to them.

The opportunity to delve into a comprehensive data set, spanning more than seven decades, is both exciting and promising. It allows for a deep exploration of trends, anomalies, and potential patterns in tornado occurrence and behavior, which can inform disaster preparedness, risk assessment, and policy development.

Data

Source

The data source for the project is comes from - TidyTuesday Archive data project, and the data set used is the Tornado Data.

```{r, echo = F,message = F, warning = F}

library(tidyverse)

#read the data

tornado <-read.csv("https://www.spc.noaa.gov/wcm/data/1950-2022_actual_tornadoes.csv")

```

Description

This data set provides comprehensive information about tornadoes, including various attributes such as tornado numbers, years, months, days, dates, times, geographical coordinates, magnitudes, injuries, fatalities, property losses, and more. The data offers insights into the characteristics and impacts of tornadoes that have occurred from 1950 to 2022.

It is sourced from NOAA's National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page (Storm Prediction).This offers a rich resource for the analysis and study of tornado occurrences and their impacts, enabling researchers and meteorologists to gain insights into the patterns and characteristics of tornadoes spanning over seven decades.

## Methods

libraries

```{r}
library(tidyverse)
library(stringr)

```

**Data Cleaning and Pre-processing**

1.  Conduct data cleaning to handle missing values, outliers, and inconsistencies in the tornado data set, ensuring that the data is of high quality for analysis.

- Print the data information

```{r, echo = F,message = F, warning = F}
#print the data information
str(tornado)

```

The data.frame "tornado" has 29 variables and 68,701 observations.

- Check for missing values in the data set.

```{r}
# Check for missing values in each column
missing_values <- colSums(is.na(tornado))

# Print the results
print(missing_values)

```

It looks like the output indicates that there are no missing values in any of the columns of your "tornado" data frame. Each column has a count of 0, meaning there are no missing values.

- Identify and Handling Inconsistent Values

```{r}
summary(tornado)
```

some potential issues or considerations based on the summary:

1. "mag" (Magnitude) Column:

The minimum value is -9, which may be inconsistent since magnitude is typically a non-negative value.

Print rows which have negative magnitude.
   
```{r}
# Identify rows where magnitude is negative

negative_mag_rows <- tornado[tornado$mag < 0, ] 
print(negative_mag_rows[, c("om", "yr", "mo", "dy", "mag","fc")])


```

Identify years where magnitude is negative.

```{r}
# Identify years where magnitude is negative

print(unique(negative_mag_rows$yr))

```
These rows seem to represent tornadoes from year 2016-2022 where the original magnitude ("mag") is listed as -9, and the "fc" value is set to 0, according to the modification scheme (ref) indicating that no modification was made for these tornadoes. 

This output aligns where tornadoes with a F-scale rating of -9 were not modified based on property loss and path length criteria.

(https://www.spc.noaa.gov/wcm/#data
https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf)












2.   Standardize and transform data types as needed to make the data set suitable for statistical analysis.

3.   Data Pre-processing with standardize data types and formats to ensure consistency.

**Tornado Trends and Patterns**

-   Examine long-term trends in tornado frequency and intensity, and identify any significant changes over the decades. Are tornadoes becoming more frequent or intense over time.

-   Investigate seasonal and regional variations in tornado occurrences. Are there particular months or states with higher tornado activity.

-   Investigate the distribution of tornado magnitudes and their classifications.

**Tornado Impact Assessment**

-   Assess the economic and human impact of tornadoes by analyzing variables such as injuries, fatalities, property and crop losses.

-   Explore the spatial distribution of tornado impact, focusing on the geographical areas most affected by these natural disasters.

**Developing Shiny Application**

-   The Shiny application will be built to import and utilize the tornado data set.

### Data Analyzing Approach

1.  Identifying and addressing missing values in the tornado data set. This will involve techniques such as imputation or removal of rows with significant missing data. Special attention will be given to critical columns, such as **inj**, **fat**, and **mag**. And perform outlier detection to identify data points that significantly deviate from the norm

2.  Data inconsistencies, such as conflicting records or contradictory information, will be resolved. This may involve cross-referencing with external sources particularly in FIPS Codes for Counties (**f1**, **f2**, **f3**, **f4**).

3.  Additional features or variables may be created to enhance the analysis. For example, calculate the total impact (injuries, fatalities, loss) for each tornado event, create time-based variables (season, time of day).

4.  To examine long-term trends in tornado frequency and intensity, we can perform a time series analysis of tornado occurrences over the years, studying variations in the number and intensity of tornadoes.

5.  For investigating seasonal and regional variations in tornado occurrences, we can create visualizations and summaries that highlight the months and states with the highest tornado activity, using the **mo**, **st**, and **stf** columns.

6.  To investigate the distribution of tornado magnitudes, we can create histograms and box plots of the **mag** column, which represents tornado intensity. We can also examine changes in tornado magnitude over time.

7.  To assess the economic and human impact of tornadoes, we can calculate statistics such as the mean, median, and maximum values of injuries, fatalities, property losses, and crop losses. This will help identify trends and the most severely affected areas.

8.  Analyzing the geographical distribution of tornado impact can be done by mapping the areas with the highest impacts and visualizing the affected regions.

9.  With the data analysis approach mentioned earlier, we are planing to add those in to a Shiny web application with implementing a variety of interactive visualizations within the Shiny application to address each of the project's goals more effectively. For instance:

-   Time series plots for trend analysis.
-   Heatmaps, choropleth maps, or scatter plots to visualize regional and seasonal variations.
-   Box plots, histograms, and distribution curves for magnitude and impact analysis.
-   Interactive maps for visualizing tornado impact locations.

Describe any data cleaning and rearranging you needed to do to get your dataset into a workable form. Make sure to cite any packages which were important in your data cleaning process in this section. For instance, if you used dplyr, then it would be appropriate to say something like

> we used the group-apply-combine paradigm with the `dplyr` functions `group_by` and `summarize` [@dplyr-package] to generate a dataset for each day of the observation period from the 15-minute interval observations in the raw data set.

## Topic of Exploration

Here, you want to introduce the first topic you want to explore with your (newly cleaned up) data. Code to process data should be contained in chunks above this point, and those chunks should *not* be included in the report.

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

If you generate a figure, it should have a caption. Here's a demonstration of how to do that:

```{r iris-plot}
#| label: fig-iris
#| fig-width: 8
#| fig-height: 4 # Change the figure dimensions using fig-width/height
#| out-width: 80% # This changes the size of the figure as rendered in the text. 
#| fig-cap: "This figure shows the relationship between sepal width and petal width in irises. I've used geom_jitter to combat overplotting, as the data are measured in relatively consistent increments. The figure is drawn with `ggplot2` [@ggplot2-package]."
#| echo: false


data(iris)
library(ggplot2)
ggplot(iris, aes(x = Sepal.Width, y = Petal.Width, color = Species)) + 
  geom_jitter() + 
  xlab("Sepal Width (cm)") + ylab("Petal Width (cm)") + 
  ggtitle("Sepal and Petal Dimensions")
```

Then, you can reference @fig-iris in the text and the appropriate cross-reference will be generated.

You can find additional information about formatting figures generated from code in the [quarto documentation](https://quarto.org/docs/authoring/figures.html#computations).

## Additional Exploration topic

Add another topic here... as many as you desire, really. Make sure to include a transition between the two sections that connects the two with some sort of logical train of thought.

## Conclusion

Here, you want to summarize the main points of what you've learned from this investigation, in paragraph form.

## Tips

(delete this section from your report!)

Almost anything you might want to know about how to format output in quarto can be found [here](https://quarto.org/docs/authoring/markdown-basics.html). Feel free to email/come to office hours to figure out how to do XYZ - part of the goal of making you write this report is that I want you to know how to write e.g. a journal paper in Quarto as well, so now's the time to experiment.

If you want to know what the wordcount of your report is, you can run the following command in your terminal:

```         
pandoc --lua-filter wordcount.lua report.qmd
```

Notice that I have not pushed `_output/report.html` or the `_output/report_files/` folder to github - this is intentional. I have actually set `_output` to not show up in git, to encourage you all to NOT push the rendered files to github and to instead work from the markdown files directly.

You may find it cleaner to create a figure subdirectory and store any figures that aren't created by R/Python in that folder. I encourage you to organize this repository in a sensible way.
